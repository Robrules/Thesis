{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "id": "bdP5O78HISHY"
   },
   "source": [
    "# Sound Compression and Generation using Variational Autoencoders in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "id": "-RpYwCN-ISHc"
   },
   "source": [
    "### Project Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "id": "-eiJwG1_ISHd"
   },
   "source": [
    "**Task 1**: Audio Preprocessing Pipeline\n",
    "\n",
    "**Task 2**: Training/Validation Split\n",
    "\n",
    "**Task 3**: Creating Data Loaders\n",
    "\n",
    "**Task 4**: VAE Architecture and Model Creation\n",
    "\n",
    "**Task 5**: Training Loop\n",
    "\n",
    "**Task 6**: Sound Generation\n",
    "\n",
    "**Task 7**: Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "id": "NsaHowcgISHe"
   },
   "source": [
    "<img src=\"Images/vae.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "ZBkiuFjnISHf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "#progress bar manager\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "#for creating data set\n",
    "import shutil\n",
    "import random\n",
    "random.seed(5)\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pickle\n",
    "\n",
    "import soundfile as sf\n",
    "import scipy\n",
    "\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import math\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "OboHoeQWISHg"
   },
   "outputs": [],
   "source": [
    "#no of time model goes over dataset set during \n",
    "epochs = 1000\n",
    "#how many samples are taken per gradient back propagation during training\n",
    "batch_size = 64\n",
    "#to preserve reproduceability for the model training \n",
    "torch.manual_seed(17)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "id": "4FmUXEcjISHh"
   },
   "source": [
    "## Task 1 : Audio Preprocessing Pipeline\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "id": "0Frtg1LWISHh"
   },
   "source": [
    "We use the Fluent Speech Commands Dataset (https://github.com/Jakobovski/free-spoken-digit-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "s_s8UPltISHi"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1- load a file\n",
    "2- pad the signal (if necessary)\n",
    "3 - extracting log spectrogram from signal\n",
    "4 - normalise spectrogram\n",
    "5 - save the normalised spectrogram\n",
    "\n",
    "PreprocessingPipeLine\n",
    "\"\"\"\n",
    "\n",
    "class Loader:\n",
    "    \"\"\"Loader is responsible for loading an audio file.\"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate, duration, mono):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.mono = mono\n",
    "\n",
    "    def load(self, file_path):\n",
    "        #signal is a tuple, we just get the first item (signal itself)\n",
    "        signal = librosa.load(file_path,\n",
    "                              sr=self.sample_rate,\n",
    "                              duration=self.duration,\n",
    "                              mono=self.mono)[0]\n",
    "        return signal\n",
    "\n",
    "\n",
    "class Padder:\n",
    "    \"\"\"Padder is responsible to apply padding to an array. Can do different types of padding\"\"\"\n",
    "\n",
    "    def __init__(self, mode=\"constant\"):\n",
    "        self.mode = mode\n",
    "\n",
    "    # [1,2,3] -> 2 -> [0,0,1,2,3]\n",
    "    def left_pad(self, array, num_missing_items):\n",
    "        padded_array = np.pad(array,\n",
    "                              (num_missing_items, 0),\n",
    "                              mode=self.mode)\n",
    "        return padded_array\n",
    "\n",
    "    # [1,2,3] -> 2 -> [1,2,3,0,0]\n",
    "    def right_pad(self, array, num_missing_items):\n",
    "        padded_array = np.pad(array,\n",
    "                              (0, num_missing_items),\n",
    "                              mode=self.mode)\n",
    "        return padded_array\n",
    "\n",
    "\n",
    "class LogSpectrogramExtractor:\n",
    "    \"\"\"LogSpectrogramExtractor extracts log spectrograms (in dB) from a\n",
    "    time-series signal.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, frame_size, hop_length):\n",
    "        self.frame_size = frame_size\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "    def extract(self, signal):\n",
    "        stft = librosa.stft(signal,\n",
    "                            n_fft=self.frame_size,\n",
    "                            hop_length=self.hop_length)[:-1]\n",
    "        spectrogram = np.abs(stft)\n",
    "        log_spectrogram = librosa.amplitude_to_db(spectrogram)\n",
    "\n",
    "        return log_spectrogram\n",
    "\n",
    "class MinMaxNormaliser:\n",
    "    \"\"\"MinMaxNormaliser applies min max normalisation to an array. Aldo denornmalises\"\"\"\n",
    "\n",
    "    def __init__(self, min_val, max_val):\n",
    "        self.min = min_val\n",
    "        self.max = max_val\n",
    "\n",
    "    def normalise(self, array):\n",
    "        norm_array = (array - array.min()) / (array.max() - array.min())\n",
    "        norm_array = norm_array * (self.max - self.min) + self.min\n",
    "        return norm_array\n",
    "\n",
    "    def denormalise(self, norm_array, original_min, original_max):\n",
    "        array = (norm_array - self.min) / (self.max - self.min)\n",
    "        array = array * (original_max - original_min) + original_min\n",
    "        return array\n",
    "\n",
    "class Saver:\n",
    "    \"\"\"saver is responsible to save features, and the min max values.\"\"\"\n",
    "\n",
    "    def __init__(self, feature_save_dir, min_max_values_save_dir):\n",
    "        self.feature_save_dir = feature_save_dir\n",
    "        self.min_max_values_save_dir = min_max_values_save_dir\n",
    "\n",
    "    def save_feature(self, feature, file_path):\n",
    "        save_path = self._generate_save_path(file_path)\n",
    "        np.save(save_path, feature)\n",
    "        return save_path\n",
    "\n",
    "    def save_min_max_values(self, min_max_values):\n",
    "        save_path = os.path.join(self.min_max_values_save_dir,\n",
    "                                 \"min_max_values.pkl\")\n",
    "        self._save(min_max_values, save_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def _save(data, save_path):\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    def _generate_save_path(self, file_path):\n",
    "        file_name = os.path.split(file_path)[1]\n",
    "        save_path = os.path.join(self.feature_save_dir, file_name + \".npy\")\n",
    "        return save_path\n",
    "\n",
    "class PreprocessingPipeline:\n",
    "    \"\"\"PreprocessingPipeline processes audio files in a directory, applying\n",
    "    the following steps to each file:\n",
    "        1- load a file\n",
    "        2- pad the signal (if necessary)\n",
    "        3- extracting log spectrogram from signal\n",
    "        4- normalise spectrogram\n",
    "        5- save the normalised spectrogram\n",
    "    Storing the min max values for all the log spectrograms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.padder = None\n",
    "        self.extractor = None\n",
    "        self.normaliser = None\n",
    "        self.saver = None\n",
    "        self.min_max_values = {}\n",
    "        self._loader = None\n",
    "        self._num_expected_samples = None\n",
    "\n",
    "    @property\n",
    "    def loader(self):\n",
    "        return self._loader\n",
    "\n",
    "    @loader.setter\n",
    "    def loader(self, loader):\n",
    "        self._loader = loader\n",
    "        self._num_expected_samples = int(loader.sample_rate * loader.duration)\n",
    "\n",
    "    def process(self, audio_files_dir):\n",
    "        for root, _, files in os.walk(audio_files_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                if (os.path.basename(file_path)[0]!= '.'):\n",
    "                    self._process_file(file_path)\n",
    "                    print(f\"Processed file {file_path}\")\n",
    "        self.saver.save_min_max_values(self.min_max_values)\n",
    "\n",
    "    def _process_file(self, file_path):\n",
    "        signal = self.loader.load(file_path)\n",
    "        if self._is_padding_necessary(signal):\n",
    "            signal = self._apply_padding(signal)\n",
    "        feature = self.extractor.extract(signal)\n",
    "        norm_feature = self.normaliser.normalise(feature)\n",
    "        save_path = self.saver.save_feature(norm_feature, file_path)\n",
    "        self._store_min_max_value(save_path, feature.min(), feature.max())\n",
    "\n",
    "    def _is_padding_necessary(self, signal):\n",
    "        if len(signal) < self._num_expected_samples:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _apply_padding(self, signal):\n",
    "        num_missing_samples = self._num_expected_samples - len(signal)\n",
    "        padded_signal = self.padder.right_pad(signal, num_missing_samples)\n",
    "        return padded_signal\n",
    "\n",
    "    def _store_min_max_value(self, save_path, min_val, max_val):\n",
    "        self.min_max_values[save_path] = {\n",
    "            \"min\": min_val,\n",
    "            \"max\": max_val\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOP_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RryoA9s1o8E",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FRAME_SIZE = 512\n",
    "HOP_LENGTH = 128\n",
    "DURATION = 3.0  # in seconds\n",
    "SAMPLE_RATE = 11025\n",
    "MONO = True\n",
    "\n",
    "SPECTROGRAMS_SAVE_DIR = \"/Users/Robert/Desktop/Thesis/spectrograms/all\"\n",
    "MIN_MAX_VALUES_SAVE_DIR = \"/Users/Robert/Desktop/Thesis/minmax\"\n",
    "FILES_DIR = \"/Users/Robert/Desktop/Thesis/recordings\"\n",
    "\n",
    "\n",
    "SAVE_DIR_ORIGINAL = \"/Users/Robert/Desktop/Thesis/reconstructed_signals\"\n",
    "# instantiate all objects\n",
    "loader = Loader(SAMPLE_RATE, DURATION, MONO )\n",
    "padder = Padder()\n",
    "log_spectrogram_extractor = LogSpectrogramExtractor(FRAME_SIZE, HOP_LENGTH)\n",
    "min_max_normaliser = MinMaxNormaliser(0, 1)\n",
    "saver = Saver(SPECTROGRAMS_SAVE_DIR, MIN_MAX_VALUES_SAVE_DIR)\n",
    "\n",
    "preprocessing_pipeline = PreprocessingPipeline()\n",
    "preprocessing_pipeline.loader = loader\n",
    "preprocessing_pipeline.padder = padder\n",
    "preprocessing_pipeline.extractor = log_spectrogram_extractor\n",
    "preprocessing_pipeline.normaliser = min_max_normaliser\n",
    "preprocessing_pipeline.saver = saver\n",
    "\n",
    "preprocessing_pipeline.process(FILES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "id": "Xf0CTGfuISHj"
   },
   "source": [
    "## Task 2: Training/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrograms_folder = '/Users/Robert/Desktop/Thesis/spectrograms/all'\n",
    "spectrograms = os.listdir(spectrograms_folder)\n",
    "spectrograms = [spec for spec in spectrograms if spec[0]!= '.']\n",
    "random.shuffle(spectrograms)\n",
    "\n",
    " #where we store training and validation set\n",
    "train_folder = '/Users/Robert/Desktop/Thesis/spectrograms/train/root'\n",
    "val_folder = '/Users/Robert/Desktop/Thesis/spectrograms/val/root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "Dy1C6V9oISHj"
   },
   "outputs": [],
   "source": [
    "if not train_folder:\n",
    "    os.mkdir(train_folder)\n",
    "if not val_folder:\n",
    "    os.mkdir(val_folder)\n",
    "\n",
    "    \n",
    "split = round(0.2 * len(spectrograms))\n",
    "for spec in tqdm(spectrograms[:split]):\n",
    "    shutil.copy(spectrograms_folder + '/' + spec, val_folder)\n",
    "\n",
    "for spec in tqdm(spectrograms[split:]):\n",
    "    shutil.copy(spectrograms_folder + '/' + spec, train_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "id": "Or_e94zvISHk"
   },
   "source": [
    "## Task 3: Creating Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "O7U85z5rISHk"
   },
   "outputs": [],
   "source": [
    "resize_value = 256\n",
    "train = '/Users/Robert/Desktop/Thesis/spectrograms/train'\n",
    "val = '/Users/Robert/Desktop/Thesis/spectrograms/val'\n",
    "def loader(spec_path):\n",
    "    spectrogram = np.load(spec_path, allow_pickle=True)\n",
    "    spectrogram = np.array(spectrogram) \n",
    "    return spectrogram\n",
    "\n",
    "transforms_set = transforms.Compose([transforms.ToTensor(), transforms.Resize([256,256])])\n",
    "\n",
    "#loaders are efficient way of reading in batches of data, can reduce ram\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.DatasetFolder(train,loader, extensions = ['.npy',], transform=transforms_set),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "    #using shuffle introduces randomness in training, ensures not learning sequential relations between dataset for each epoch\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.DatasetFolder(val,loader, extensions = ['.npy',], transform=transforms_set),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "    #shuffle also ensures we dont see same characters each time in results\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    spec = batch[0][0]\n",
    "    print(spec.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "id": "mZ1wY9OPISHk"
   },
   "source": [
    "## Task 4: VAE Architecture and Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "9ZQ6ishlISHk"
   },
   "outputs": [],
   "source": [
    "shape = resize_value\n",
    "lower_dimension = 1000\n",
    "latent_dimension = 16\n",
    "#code based on pytorch documentaion on creating a VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "    \n",
    "        #z represents latent dimension, 32 is chosen due to good tradeoff between reconstruction loss and interesting sampling\n",
    "        #can be arbituary based on experimentation or datasets\n",
    "         \n",
    "        \n",
    "        #linear layers convert one dimensionality into another. 1st parameter is input, 2nd is output\n",
    "        # dimentionality reduced slowly\n",
    "        #we sample from mean and standarde deviation\n",
    "        self.fc1 = nn.Linear(shape*shape, lower_dimension)\n",
    "        self.fc21 = nn.Linear(lower_dimension, latent_dimension) #mean\n",
    "        self.fc22 = nn.Linear(lower_dimension, latent_dimension) #standard deviation\n",
    "        self.fc3 = nn.Linear(latent_dimension, lower_dimension)\n",
    "        self.fc4 = nn.Linear(lower_dimension, shape*shape)\n",
    "\n",
    "    #encode does what we specify above\n",
    "    def encode(self, x):\n",
    "        #creates hidden dimensionality/layer h1, does a Nonlinearity called relu, returns mean and standard deviation vectors\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "    \n",
    "    #deals with randomness and distribution that VAE includes, allows proper backpropagation\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    #similar to encode, takes z dimensions and increases dimensionality\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):   \n",
    "        mu, logvar = self.encode(x.view(-1, shape*shape)) #re-adjusts shape to be flat\n",
    "            \n",
    "        #creates latent space\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        print(z)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "MERarDFhISHl"
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    try:  \n",
    "        BCE = F.binary_cross_entropy(recon_x, x.view(-1, shape*shape), reduction='sum')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "JKB5VsoKISHl"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#moves model onto correct device (cpu or gpu)\n",
    "model = VAE().to(device)\n",
    "#adjusts weights whenever we backpropagate, default learning rate is used\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "id": "h0HKPumVISHl"
   },
   "source": [
    "## Task 5: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "LobsJ5vmISHl"
   },
   "outputs": [],
   "source": [
    "#after each epoc, generate validation peformance metric to see how model performs on validation data alongside training data\n",
    "#important to avoid overtraining and to know when model knowns enough or meets expectations\n",
    "def evaluate(evaluate_data=val_loader):\n",
    "    #put model into evaluate mode to ensure gradients are frozen and to avoid doing any training - \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    #not changing gradients with forward passes\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(evaluate_data):\n",
    "            \n",
    "            data = data.to(device)\n",
    "            #run model, return reconstructed spectrogram, mean and variance vectors\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            \n",
    "            #update validation loss\n",
    "            try:          \n",
    "                val_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            except:\n",
    "                #print(\"val error\")\n",
    "                #val_loss+=0\n",
    "                pass\n",
    "\n",
    "    #divide validation loss by size of all batches added up to get average loss\n",
    "    val_loss /= len(evaluate_data.dataset)\n",
    "    return val_loss\n",
    "\n",
    "#sample latent space model has learnt and generate new spectrograms\n",
    "def sample_latent_space():\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #sample from normal distribution - 1 spectrogram, 32 is our hidden dimension. So 1 vector of 32 dimensions \n",
    "        sample = torch.randn(1, 16).to(device)\n",
    "        #avoid encoder and go to middle of model to decode data\n",
    "        sample = model.decode(sample).cpu()\n",
    "        \n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "-Bx5DINWISHm"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "\n",
    "    #set to train mode so certain layers can act as intended\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    #show what epoch we're on\n",
    "    progress_bar = tqdm(train_loader, desc='Epoch {:03d}'.format(epoch), leave=False, disable=False)\n",
    "    for data,_ in progress_bar:\n",
    "\n",
    "        data = data.to(device)\n",
    "        \n",
    "        #set gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #run model, return reconstructed spectrogram batch, mean and variance vectors\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        try:   \n",
    "            loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            #add training loss to progress bar for each loop\n",
    "            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(data))})\n",
    "            #calculate gradients in order to do backwards propagation\n",
    "        except:\n",
    "            #print(\"train error\")\n",
    "            #train_loss += 0\n",
    "            pass\n",
    "    \n",
    "\n",
    "        #update training loss with current training loss\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "    average_train_loss = train_loss / len(train_loader.dataset)\n",
    "    tqdm.write('Training set loss (average, epoch {:03d}): {:.3f}'.format(epoch, average_train_loss))\n",
    "    val_loss = evaluate(val_loader)\n",
    "    tqdm.write('\\t\\t\\t\\t====> Validation set loss: {:.3f}'.format(val_loss))\n",
    "\n",
    "    train_losses.append(average_train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    #save model state dictionairy every X epochs, which contains all layers, layer weights necessary to load in fresh model\n",
    "    if epoch%50==0:\n",
    "        torch.save(model.state_dict(), f'/Users/Robert/Desktop/thesis/models/epoch_{epoch}.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "zHyhYCQJISHm"
   },
   "outputs": [],
   "source": [
    "epochs=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "TS9hiq_rISHm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch)\n",
    "    #sample_latent_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/Users/Robert/Desktop/thesis/training_losses.txt', np.array(train_losses), delimiter='\\n')\n",
    "np.savetxt('/Users/Robert/Desktop/thesis/validation_losses.txt', np.array(val_losses), delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "M-qgjFkOISHm"
   },
   "outputs": [],
   "source": [
    "#You can use this to load model states that have been saved\n",
    "#does not need to be ran if you just trained the model\n",
    "model.load_state_dict(torch.load('/Users/Robert/Desktop/Thesis/models/model11/epoch_1000.model', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "QOOTL8nGISHm"
   },
   "outputs": [],
   "source": [
    "#just shows output of a latent representations, this is an example of what is eventually conveted to a signal\n",
    "sample_latent_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JN1yMLEWb59T"
   },
   "source": [
    "## Task 6: Sound Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilh7RLeDb-1A"
   },
   "outputs": [],
   "source": [
    "class SoundGenerator:\n",
    "    \n",
    "  #Responsible for generating audios from spectrograms\n",
    "\n",
    "\n",
    "    def __init__(self, vae, hop_length):\n",
    "        \n",
    "        self.vae = vae\n",
    "        self.hop_length = hop_length #needed to pass from spectrogram to signal wave form\n",
    "        self._min_max_normaliser = MinMaxNormaliser(0,1)# needed to denormalise spectrograms\n",
    "\n",
    "    def generate(self, spectrograms, min_max_values):\n",
    "\n",
    "        #convert to signals\n",
    "        #returns signals and latent representations\n",
    "        \n",
    "        \n",
    "        #input sampled spectrograms into model and generate reconstructed specotrgrams\n",
    "        generated_specs = []\n",
    "        for spec in spectrograms:\n",
    "            g_spec,_,_ = model(spec)\n",
    "            generated_specs.append(g_spec)\n",
    "        \n",
    "\n",
    "        #generate signals based on generated specs and min_max_values\n",
    "        signals = self.convert_spectrograms_to_audio(generated_specs, min_max_values, False)\n",
    "\n",
    "        return signals\n",
    "\n",
    "    def convert_spectrograms_to_audio(self, spectrograms, min_max_values, original):\n",
    "        signals = []\n",
    "        for log_spectrogram, min_max_value in zip(spectrograms, min_max_values):\n",
    "            if original == True:\n",
    "                #print(\"original signal\",log_spectrogram.shape)\n",
    "                #log_spectrogram = log_spectrogram.cpu().detach().numpy()\n",
    "                print(\"original signal\",log_spectrogram.shape)\n",
    "                \n",
    "            elif original == False:\n",
    "                log_spectrogram = log_spectrogram.view(256,256)\n",
    "                log_spectrogram = log_spectrogram.cpu().detach().numpy()\n",
    "                print(\"generated signal\",log_spectrogram.shape)\n",
    "            #apply denormalisation\n",
    "            denorm_log_spec = self._min_max_normaliser.denormalise(log_spectrogram, min_max_value[\"min\"],min_max_value[\"max\"])\n",
    "            #convert log spectrogram to normal spectrogram\n",
    "            spec = librosa.db_to_amplitude(denorm_log_spec)\n",
    "            #apply Griffin-Lim (inverse short time fourier transform)\n",
    "            signal = librosa.griffinlim(spec)\n",
    "            #append signal to \"signals\"\n",
    "            signals.append(signal)\n",
    "            \n",
    "            \n",
    "        return signals\n",
    "\n",
    "            \n",
    "    def convert_latent_to_audio(self, spec):\n",
    "        signals = []\n",
    "\n",
    "        spec = spec.view(256,256 )\n",
    "        spec = spec.cpu().detach().numpy()\n",
    "        \n",
    "        print(\"latent representation\",spec.shape)\n",
    "        spec = self._min_max_normaliser.denormalise(spec, -46.56255,33.43745)\n",
    "        spec = librosa.db_to_amplitude(spec)\n",
    "        #apply Griffin-Lim (inverse short time fourier transform)\n",
    "        signal = librosa.griffinlim(spec)\n",
    "        signals.append(signal)\n",
    "        return signals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZF87GSHKUZgn"
   },
   "outputs": [],
   "source": [
    "SPECTROGRAMS_PATH = \"/Users/Robert/Desktop/Thesis/spectrograms/all\"\n",
    "MIN_MAX_VALUES_PATH = \"/Users/Robert/Desktop/Thesis/minmax/min_max_values.pkl\"\n",
    "\n",
    "SAVE_DIR_ORIGINAL = \"/Users/Robert/Desktop/Thesis/reconstructed_signals/original\"\n",
    "SAVE_DIR_GENERATED = \"/Users/Robert/Desktop/Thesis/reconstructed_signals/generated\"\n",
    "SAVE_DIR_LATENT = \"/Users/Robert/Desktop/Thesis/reconstructed_signals/latent\"\n",
    "\n",
    "#load spectrograms\n",
    "def load_specs(spectrograms_path):\n",
    "    specs = []\n",
    "    file_paths = []\n",
    "    for root, _, file_names in os.walk(spectrograms_path):\n",
    "        for file_name in file_names:\n",
    "            if (file_name[0] != '.'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                spectrogram = np.load(file_path,allow_pickle=True) # (n_bins, n_frames, 1) \n",
    "                specs.append(spectrogram)\n",
    "                file_paths.append(file_path)      \n",
    "    specs = np.array(specs)\n",
    "    #specs = specs[...,np.newaxis]\n",
    "    return specs, file_paths\n",
    "\n",
    "#save signal\n",
    "def save_signals(signals, save_dir, sample_rate=11025):\n",
    "    for i, signal in enumerate(signals):\n",
    "        save_path = os.path.join(save_dir, str(i) + \".wav\")\n",
    "        sf.write(save_path, signal, sample_rate)\n",
    "\n",
    "#select sample of spectrograms and associated min-max values\n",
    "def select_spectrograms(spectrograms,\n",
    "                        file_paths,\n",
    "                        min_max_values,\n",
    "                        num_spectrograms):\n",
    "    sampled_indexes = np.random.choice(range(len(spectrograms)), num_spectrograms)\n",
    "    sampled_spectrograms = spectrograms[sampled_indexes]\n",
    "    file_paths = [file_paths[index] for index in sampled_indexes]\n",
    "    \n",
    "    sampled_min_max_values = [min_max_values[file_path] for file_path in\n",
    "                           file_paths]\n",
    "\n",
    "    return sampled_spectrograms, sampled_min_max_values, file_paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise sound generator\n",
    "sound_generator = SoundGenerator(model,HOP_LENGTH)\n",
    "\n",
    "#load min max values\n",
    "with open(MIN_MAX_VALUES_PATH, \"rb\") as f:\n",
    "        min_max_values = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load spectrograms\n",
    "specs, file_paths = load_specs(SPECTROGRAMS_PATH)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_file = \"/Users/Robert/Desktop/Thesis/spectrograms/all/c0e37430-45e5-11e9-b578-494a5b19ab8b.wav.npy\"\n",
    "higher_pitch = \"/Users/Robert/Desktop/Thesis/spectrograms/all/higher.wav.npy\"\n",
    "\n",
    "female_file = \"/Users/Robert/Desktop/Thesis/spectrograms/all/643ad320-45d8-11e9-81ce-69b74fd7e64e.wav.npy\"\n",
    "lower_pitch = \"/Users/Robert/Desktop/Thesis/spectrograms/all/lower.wav.npy\"\n",
    "\n",
    "sampled_min_max_values = [min_max_values[female_file]]\n",
    "print(sampled_min_max_values)\n",
    "\n",
    "sampled_specs = []\n",
    "random_spec = np.load(female_file)\n",
    "sampled_specs.append(random_spec)\n",
    "\n",
    "tensor_specs = []\n",
    "tensor_specs.append(transforms_set(random_spec))\n",
    "\n",
    "#convert original signals to audio (skipping the model, for comparison)\n",
    "original_signals = sound_generator.convert_spectrograms_to_audio(sampled_specs, sampled_min_max_values, True)\n",
    "generated_signals = sound_generator.generate(tensor_specs, sampled_min_max_values)\n",
    "\n",
    "save_signals(original_signals, SAVE_DIR_ORIGINAL)\n",
    "save_signals(generated_signals, SAVE_DIR_GENERATED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMgnAHz8OTgZ"
   },
   "outputs": [],
   "source": [
    "#Sample a number of spectrograms from the dataset\n",
    "sampled_specs, sampled_min_max_values,f_paths = select_spectrograms(specs,\n",
    "                                                                file_paths,\n",
    "                                                                min_max_values,\n",
    "                                                               1)\n",
    "\n",
    "#perform required transformations on sampled specs for model\n",
    "tensor_specs = []\n",
    "for spec in sampled_specs:\n",
    "    spec = transforms_set(spec)\n",
    "    tensor_specs.append(spec)\n",
    "\n",
    "\n",
    "#input spectrograms into the model and generate signals   \n",
    "generated_signals = sound_generator.generate(tensor_specs, sampled_min_max_values)\n",
    "\n",
    "#convert original signals to audio (skipping the model, for comparison)\n",
    "original_signals = sound_generator.convert_spectrograms_to_audio(sampled_specs, sampled_min_max_values, True)\n",
    "\n",
    "\n",
    "save_signals(original_signals, SAVE_DIR_ORIGINAL)\n",
    "save_signals(generated_signals, SAVE_DIR_GENERATED)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_representations = sample_latent_space()\n",
    "latent_signals = sound_generator.convert_latent_to_audio(latent_representations)\n",
    "save_signals(latent_signals, SAVE_DIR_LATENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE\n",
    "#bedroom lights on\n",
    "male_array = [[-0.5781,  1.7825, -4.4770,  2.3321, -1.5133,  1.5367,  2.9303,  1.9751,\n",
    "         -2.7641, -2.3457, -2.0747, -0.4873,  2.6040,  6.0554, -4.8897,  1.5923]]\n",
    "\n",
    "male_array_pitchedup = [[2.7032,  0.2202, -3.1795,  3.4626, -3.0187,  2.4574,  0.7890, -1.0400,\n",
    "          0.2349, -1.6246, -2.2396, -3.7486,  0.3878,  5.2447, -7.4049,  1.8736]]\n",
    "\n",
    "#modify male array by change top 3 most relevant features (1 & 12 changed, 8 modified)\n",
    "result = [[2.7032,  1.7825, -4.4770,  2.3321, -1.5133,  1.5367,  2.9303,  0.55,\n",
    "         -2.7641, -2.3457, -2.0747, -3.7486,  2.6040,  6.0554, -4.8897,  1.5923]]\n",
    "new_vector = torch.Tensor(result)\n",
    "print(new_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE\n",
    "#Make the music softer\n",
    "male_array = [[2.3603, -0.8928,  3.1577,  3.1412,  3.3199,  0.0068,  0.0714, -0.1817,\n",
    "          1.5153,  0.6460, -0.8865, -2.8240,  1.1089, -2.9000, -5.1773, -0.3459]]\n",
    "\n",
    "male_array_pitchedup = [[3.8991, -1.4469,  2.5336,  1.8094,  1.5834, -2.4974, -1.8551, -3.9157,\n",
    "          3.0788,  0.5842, -2.0938, -4.3731,  2.0327, -0.8192, -3.6701, -2.7992]]\n",
    "\n",
    "#a = male_array[0]\n",
    "#b = male_array_pitchedup[0]\n",
    "#result = [[statistics.mean(k) for k in zip(a,b)]]  \n",
    "#modify male array and modify 8th variable with pitched\n",
    "result = [[2.3603, -0.8928,  3.1577,  3.1412,  3.3199,  0.0068,  0.0714, -3.25,\n",
    "          1.5153,  0.6460, -0.8865, -2.8240,  1.1089, -2.9000, -5.1773, -0.3459]]\n",
    "new_vector = torch.Tensor(result)\n",
    "print(new_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE\n",
    "#Switch on the kitchen lights\n",
    "male_array = [[1.7631, -2.7329,  2.1709,  1.4151,  2.0126,  4.3662, -1.8836, -1.2619,\n",
    "          3.5545,  1.5870, -0.8313, -0.7222, -0.2389,  0.1972, -1.1918, -6.0831]]\n",
    "\n",
    "male_array_pitchedup = [[1.3993, -1.7566,  1.5705,  1.7905,  0.8157,  1.6816, -3.2363, -0.4246,\n",
    "          4.3625,  0.1268, -2.1645, -2.5726,  2.3026,  0.9361,  2.3026, -7.5896]]\n",
    " \n",
    "#modify male array and modify 13th,15th variable with pitched\n",
    "result = [[1.7631, -2.7329,  2.1709,  1.4151,  2.0126,  4.3662, -1.8836, -1.2619,\n",
    "          3.5545,  1.5870, -0.8313, -0.7222, 2.3026,  0.1972, 0.61, -6.0831]]\n",
    "new_vector = torch.Tensor(result)\n",
    "print(new_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE\n",
    "#Turn off the lights in the bedroom\n",
    "female_array = [[5.0933, -1.6751, -1.5892, -3.3666,  0.9798, -1.3267,  0.8459, -6.3412,\n",
    "         -0.6881, -1.1514,  0.8595, -1.1106,  3.4330, -0.2548,  1.1554, -0.5961]]\n",
    "\n",
    "female_array_pitcheddown = [[0.1500, -5.5779, -5.0892, -2.7831,  4.4215, -5.3856,  1.2228, -2.1289,\n",
    "         -1.9717, -2.2412, -2.7855, -1.8682,  5.4151,  6.1053,  0.5474, -0.7980]]\n",
    "\n",
    "#modify female array by 1,2,3,4,5,6,8,11,13\n",
    "result = [[0.1500, -5.5779, -5.0892, -3.3666,  4.4215, -5.3856,  0.8459, -2.1289,\n",
    "         -0.6881, -1.1514, -2.7855, -1.1106,  4.0,  6.1053,  1.1554, -0.5961]]\n",
    "new_vector = torch.Tensor(result)\n",
    "print(new_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switch on the bathroom lights\n",
    "female_array = [[1.2856, -3.7600,  5.1191, -0.8282, -1.0653, -3.9675,  2.4507,  0.7030,\n",
    "         -0.9069, -0.7750,  0.9804, -3.7015, -0.9143, -1.6186, -0.4156,  3.6323]]\n",
    "\n",
    "female_array_pitcheddown = [[-1.4646, -6.1295,  1.0632, -2.6649,  5.0519, -6.7785,  3.1655, -0.0434,\n",
    "         -1.3328, -4.0500, -3.3925, -3.2585,  1.0946,  2.9671, -0.9856,  3.5367]]\n",
    "\n",
    "#modified female array by 1,2,3,4,5,9,10,12,13,\n",
    "result = [[-1.4646, -6.1295,  1.0632, -0.8282,  5.0519, -6.7785,  2.4507, 0.7030,\n",
    "         -0.9069, -4.0500, -3.3925, -3.7015,  1.0946,  2.9671, -0.4156,  3.6323]]  \n",
    "new_vector = torch.Tensor(result)\n",
    "print(new_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Increase the temperature in the kitchen\n",
    "female_array = [[-0.7386, -4.2983,  2.2131, -2.4853,  3.2881,  3.2733,  3.0581, -0.9935,\n",
    "         -1.2309, -1.7480,  2.6251, -1.9296,  1.0876,  1.0788, -1.3025,  3.2276]]\n",
    "\n",
    "female_array_pitcheddown = [[-3.7020, -5.0420, -1.7487, -4.2840,  6.4130, -1.9832,  3.5992, -1.1902,\n",
    "         -0.8086, -3.2695, -1.9878, -1.1099,  2.9419,  6.4888, -0.4898,  1.1227]]\n",
    "\n",
    "#changes from pitched: 8,9,7,2,15, 12,10,13\n",
    "result = [[-3.7020, -4.2983, -1.7487, -4.2840,  6.4130, -1.9832,  3.0581, -0.9935,\n",
    "         -1.2309, -1.7480, -1.9878, -1.9296,  2.9419,  6.4888, -1.3025,  1.1227]]\n",
    "new_vector = torch.Tensor(result)\n",
    "print(new_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    #sample from normal distribution - 1 spectrogram, 32 is our hidden dimension. So 1 vector of 32 dimensions \n",
    "    #sample = torch.randn(1, 16).to(device)\n",
    "    #vector = sample\n",
    "    #avoid encoder and go to middle of model to decode data\n",
    "    sample = model.decode(new_vector).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_signals = sound_generator.convert_latent_to_audio(sample)\n",
    "save_signals(latent_signals, SAVE_DIR_LATENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "id": "erY5he78ISHn"
   },
   "source": [
    "## Task 7: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "R12q3eF3ISHn"
   },
   "outputs": [],
   "source": [
    "train_losses = np.loadtxt('/Users/Robert/Desktop/thesis/training_losses.txt')\n",
    "val_losses = np.loadtxt('/Users/Robert/Desktop/thesis/validation_losses.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "icDHZ0vAISHn"
   },
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    range(1, len(train_losses)+1), \n",
    "    train_losses, \n",
    "    label='Training Loss',\n",
    "    linewidth=2, \n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    range(1, len(val_losses)+1),\n",
    "    val_losses,\n",
    "    label='Validation Loss',\n",
    "    linewidth=2,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "plt.title('VAE Spectrogram Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "_lmUbPJ3ISHo"
   },
   "outputs": [],
   "source": [
    "original_file = \"/Users/Robert/Desktop/Thesis/reconstructed_signals/original/0.wav\"\n",
    "generated_file = \"/Users/Robert/Desktop/Thesis/reconstructed_signals/generated/0.wav\"\n",
    "latent_file = \"/Users/Robert/Desktop/Thesis/reconstructed_signals/latent/0.wav\"\n",
    "\n",
    "original, sr = librosa.load(original_file)\n",
    "generated, _ = librosa.load(generated_file)\n",
    "latent, _ = librosa.load(latent_file)\n",
    "#extract short term fourier transform\n",
    "S_original = librosa.stft(original, n_fft=2048, hop_length=HOP_LENGTH)\n",
    "S_generated = librosa.stft(generated, n_fft=2048, hop_length=HOP_LENGTH)\n",
    "S_latent = librosa.stft(latent, n_fft=2048, hop_length=HOP_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(original_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(generated_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(latent_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise spectrogram\n",
    "def plot_spectrogram(Y, sr, hop_length, y_axis=\"linear\"):\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    librosa.display.specshow(Y, \n",
    "                             sr=sr, \n",
    "                             hop_length=hop_length, \n",
    "                             x_axis=\"time\", \n",
    "                             y_axis=y_axis)\n",
    "    plt.colorbar(format=\"%+2.f\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_log_original = librosa.power_to_db(np.abs(S_original) ** 2)\n",
    "Y_log_generated = librosa.power_to_db(np.abs(S_generated) ** 2)\n",
    "Y_log_latent = librosa.power_to_db(np.abs(S_latent) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 40})\n",
    "#log frequency spectrogram\n",
    "#original signal\n",
    "plot_spectrogram(Y_log_original, sr, HOP_LENGTH, y_axis=\"log\")\n",
    "plt.savefig(\"original.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generated/reconstructed spectrogram\n",
    "plot_spectrogram(Y_log_generated, sr, HOP_LENGTH, y_axis=\"log\")\n",
    "plt.savefig(\"generated.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spectrogram sampled from latent space\n",
    "plot_spectrogram(Y_log_latent, sr, HOP_LENGTH, y_axis=\"log\")\n",
    "plt.savefig(\"latent.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Oq4zRdAbISHb"
   ],
   "name": "Complete-Sound.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
